import os
import requests
from bs4 import BeautifulSoup
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import SupabaseVectorStore
# âœ… Correct API Embeddings
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_core.documents import Document
from supabase import create_client
from dotenv import load_dotenv

# 1. Load Environment Variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("âŒ SUPABASE_URL or SUPABASE_KEY is missing in .env file")

if not HF_TOKEN:
    raise ValueError("âŒ HUGGINGFACEHUB_API_TOKEN is missing. Add it to .env file.")

# 2. Setup Supabase Client
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Define Embeddings (API Version - 384 Dimensions)
print("ğŸ§  Initializing HuggingFace Cloud Embeddings...")
embeddings = HuggingFaceEndpointEmbeddings(
    model="sentence-transformers/all-MiniLM-L6-v2",
    huggingfacehub_api_token=HF_TOKEN
)

# --- DATA SOURCE: HARDCODED RULES ---
CHARUSAT_KNOWLEDGE_BASE = """
SOURCE: CHARUSAT_OFFICIAL_RULES
TYPE: UNIVERSITY_SCHOLARSHIP

1. Charusat Merit Scholarship (For B.Tech / Engineering):
   - Eligibility is based on HSC / JEE Main / GUJCET performance.
   - Renewal Criteria: 
     - If CGPA >= 9.0: 100% Tuition Fee Waiver.
     - If CGPA >= 8.5 and < 9.0: 50% Tuition Fee Waiver.
   - This scholarship is applicable for students of CSPIT and DEPSTAR colleges.

2. MYSY (Mukhymantri Yuva Swavalamban Yojana):
   - Government of Gujarat Scheme available for Charusat students.
   - Benefit: 50% of Tuition Fees or Rs. 50,000 (whichever is less).
   - Eligibility: 80+ Percentile in 12th Standard.
   - Income Limit: Family income must be less than Rs. 6 Lakh/annum.
"""

def ingest_data():
    print("ğŸš€ Starting Data Ingestion via API (PDFs + Text + Web)...")
    documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    # --- PHASE 1: Internal Knowledge Base ---
    print("ğŸ”¹ Processing Internal Knowledge Base...")
    docs = text_splitter.create_documents(
        [CHARUSAT_KNOWLEDGE_BASE], 
        metadatas=[{"source": "charusat_rules", "type": "hardcoded_rules"}]
    )
    documents.extend(docs)

    # --- PHASE 2: Web Scraping ---
    print("ğŸ”¹ Scraping Charusat Website...")
    target_url = "https://www.charusat.ac.in/scholarship.php"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(target_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            text_content = soup.get_text()
            if text_content.strip():
                web_docs = text_splitter.create_documents(
                    [text_content],
                    metadatas=[{"source": "charusat.ac.in", "type": "web_scrape"}]
                )
                documents.extend(web_docs)
                print(f"   âœ… Scraped {len(web_docs)} chunks from website.")
    except Exception as e:
        print(f"   âš ï¸ Scraping skipped: {e}")

    # --- PHASE 3: PDF Ingestion ---
    pdf_dir = "data/charusat_pdfs"
    if os.path.exists(pdf_dir):
        print(f"ğŸ”¹ Checking for PDFs in {pdf_dir}...")
        for filename in os.listdir(pdf_dir):
            if filename.endswith(".pdf"):
                file_path = os.path.join(pdf_dir, filename)
                print(f"   ğŸ“„ Processing PDF: {filename}")
                try:
                    loader = PyPDFLoader(file_path)
                    pdf_pages = loader.load()
                    pdf_chunks = text_splitter.split_documents(pdf_pages)
                    for chunk in pdf_chunks:
                        chunk.metadata["source"] = "charusat_pdf"
                        chunk.metadata["filename"] = filename
                    documents.extend(pdf_chunks)
                    print(f"   âœ… Extracted {len(pdf_chunks)} chunks.")
                except Exception as e:
                    print(f"   âŒ Error reading PDF {filename}: {e}")

    # --- PHASE 4: Text File Ingestion ---
    txt_dir = "data"
    if os.path.exists(txt_dir):
        print(f"ğŸ”¹ Checking for Text (.txt) files in {txt_dir}...")
        for filename in os.listdir(txt_dir):
            if filename.endswith(".txt"):
                file_path = os.path.join(txt_dir, filename)
                print(f"   ğŸ“ Processing Text File: {filename}")
                try:
                    loader = TextLoader(file_path, encoding='utf-8')
                    txt_docs = loader.load()
                    txt_chunks = text_splitter.split_documents(txt_docs)
                    
                    for chunk in txt_chunks:
                        chunk.metadata["source"] = "text_file"
                        chunk.metadata["filename"] = filename
                        
                    documents.extend(txt_chunks)
                    print(f"   âœ… Extracted {len(txt_chunks)} chunks.")
                except Exception as e:
                    print(f"   âŒ Error reading Text file {filename}: {e}")

    # --- PHASE 5: Upload to Vector DB ---
    if documents:
        print(f"\nğŸ“¦ Uploading {len(documents)} document chunks to Supabase Vector Store...")
        print("â³ using Cloud API for Embeddings...")
        try:
            vector_store = SupabaseVectorStore(
                client=supabase,
                embedding=embeddings,
                table_name="documents",
                query_name="match_documents"
            )
            
            # Batch upload logic
            batch_size = 50
            total_docs = len(documents)
            for i in range(0, total_docs, batch_size):
                batch = documents[i:i + batch_size]
                vector_store.add_documents(batch)
                print(f"   âœ… Uploaded batch {i//batch_size + 1}")
            
            print("ğŸ‰ SUCCESS: All Data Ingested & Synced via API!")
        except Exception as e:
            print(f"âŒ Upload Failed: {e}")
    else:
        print("âš ï¸ No data found to ingest.")

if __name__ == "__main__":
    ingest_data()